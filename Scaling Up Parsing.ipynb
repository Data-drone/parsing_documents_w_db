{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "\n",
    "# Scaling up parsing routines\n",
    "\n",
    "Lets look at how to scale up parsing leveraging a spark cluster\n",
    "\n",
    "As a general guidance if you have less than ?couple of dozen? files then it may not be worth distributing across a cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pymupdf4llm langchain databricks-langchain\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymupdf4llm\n",
    "\n",
    "catalog = 'brian_ml_dev'\n",
    "schema = 'parsing_tests'\n",
    "volume = 'raw_data'\n",
    "\n",
    "llm_model_name = 'brian_serving_test'\n",
    "llm_model_name = 'databricks-meta-llama-3-1-70b-instruct'\n",
    "\n",
    "full_vol_path = f'/Volumes/{catalog}/{schema}/{volume}'\n",
    "\n",
    "example_files = [f for f in os.listdir(f'{full_vol_path}')  if f.endswith('.pdf')]\n",
    "example_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse all files into a dataframe\n",
    "\n",
    "raw_files_df = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .option(\"pathGlobFilter\", f\"*.pdf\")\n",
    "    .load(full_vol_path)\n",
    ")\n",
    "\n",
    "display(raw_files_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Function\n",
    "\n",
    "At it's most basic level a parsing function just takes an input and produces an output\n",
    "\n",
    "In this case we get a filepath and we output markdown text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pymupdf_parse(file_path: str) -> str:\n",
    "    markdown_text = pymupdf4llm.to_markdown(file_path)\n",
    "    \n",
    "    return markdown_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to convert this to a pyspark udf in order to distribute it on spark\n",
    "\n",
    "This requires that we set it up with an output signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# for out basic example, the types are: string\n",
    "\n",
    "pymupdf_parse_udf = F.udf(\n",
    "    pymupdf_parse,\n",
    "    returnType=T.StringType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_files = raw_files_df \\\n",
    "    .withColumn('file_path', F.substring('path', 6, F.length('path'))) \\\n",
    "    .withColumn('markdown', pymupdf_parse_udf(\"file_path\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(parsed_files)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
